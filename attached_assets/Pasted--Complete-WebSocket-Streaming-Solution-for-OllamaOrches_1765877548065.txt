# Complete WebSocket Streaming Solution for OllamaOrchestrator

## Problem Analysis Confirmed

Your current architecture has TWO critical bottlenecks:

### Current Flow (SLOW - 1.5-2s first token)
```
Browser → SSE endpoint → Creates job → Agent polls (500ms) → Ollama (1-1.5s) → HTTP POST per chunk (80-120ms) → Server → SSE → Browser
```

**Total Delays:**
- Agent polling: **0-500ms** (random wait time)
- Ollama cold start: **800-1500ms** (unavoidable)
- HTTP POST overhead: **80-120ms PER CHUNK** (huge bottleneck!)

### Optimal Flow (FAST - ~1s first token)
```
Browser → SSE endpoint → WebSocket push → Agent (instant) → Ollama (1-1.5s) → WebSocket stream → Server → SSE → Browser
```

**Eliminated Delays:**
- Agent polling: **0ms** (instant push)
- HTTP POST overhead: **0ms** (persistent connection)
- Only Ollama generation remains (unavoidable)

---

## Complete Implementation

### Architecture Overview

```
┌─────────┐
│ Browser │
└────┬────┘
     │ SSE
     ↓
┌─────────────┐     WebSocket (Bidirectional)     ┌────────┐
│   Server    │ ←──────────────────────────────→  │ Agent  │
│  (Node.js)  │                                    │(Python)│
└─────────────┘                                    └───┬────┘
     ↑                                                  │
     │ SSE                                             │ HTTP
     │                                                  ↓
┌─────────┐                                       ┌────────┐
│ Browser │                                       │ Ollama │
└─────────┘                                       └────────┘
```

### Part 1: Server-Side WebSocket Manager

```typescript
// server/services/AgentConnectionManager.ts

import { WebSocketServer, WebSocket } from 'ws';
import crypto from 'crypto';
import { EventEmitter } from 'events';

interface AgentInfo {
  id: string;
  ws: WebSocket;
  models: string[];
  status: 'idle' | 'busy';
  lastHeartbeat: Date;
  activeJobs: Set<string>;
}

interface JobRequest {
  jobId: string;
  model: string;
  prompt: string;
  parameters?: any;
}

interface TokenChunk {
  jobId: string;
  token: string;
  done: boolean;
}

export class AgentConnectionManager extends EventEmitter {
  private agents: Map<string, AgentInfo> = new Map();
  private wss: WebSocketServer;
  private heartbeatInterval: NodeJS.Timeout;

  constructor(port: number = 8081) {
    super();
    
    this.wss = new WebSocketServer({ port });
    
    this.wss.on('connection', (ws: WebSocket) => {
      this.handleAgentConnection(ws);
    });

    // Heartbeat check every 10 seconds
    this.heartbeatInterval = setInterval(() => {
      this.checkHeartbeats();
    }, 10000);

    console.log(`Agent WebSocket server listening on port ${port}`);
  }

  private handleAgentConnection(ws: WebSocket) {
    const agentId = crypto.randomBytes(16).toString('hex');
    let agentInfo: AgentInfo | null = null;

    ws.on('message', (data: Buffer) => {
      try {
        const message = JSON.parse(data.toString());
        this.handleAgentMessage(agentId, message, ws);
      } catch (error) {
        console.error('Failed to parse agent message:', error);
      }
    });

    ws.on('close', () => {
      console.log(`Agent ${agentId} disconnected`);
      this.agents.delete(agentId);
      this.emit('agent:disconnected', agentId);
    });

    ws.on('error', (error) => {
      console.error(`Agent ${agentId} error:`, error);
    });

    // Send welcome message with agent ID
    ws.send(JSON.stringify({
      type: 'welcome',
      agentId,
      timestamp: Date.now(),
    }));
  }

  private handleAgentMessage(agentId: string, message: any, ws: WebSocket) {
    switch (message.type) {
      case 'register':
        this.registerAgent(agentId, ws, message.models);
        break;

      case 'heartbeat':
        this.updateHeartbeat(agentId);
        break;

      case 'token':
        this.handleToken(agentId, message);
        break;

      case 'job_complete':
        this.handleJobComplete(agentId, message.jobId);
        break;

      case 'job_error':
        this.handleJobError(agentId, message);
        break;

      case 'status':
        this.updateAgentStatus(agentId, message.status);
        break;

      default:
        console.warn(`Unknown message type from agent ${agentId}:`, message.type);
    }
  }

  private registerAgent(agentId: string, ws: WebSocket, models: string[]) {
    const agentInfo: AgentInfo = {
      id: agentId,
      ws,
      models: models || [],
      status: 'idle',
      lastHeartbeat: new Date(),
      activeJobs: new Set(),
    };

    this.agents.set(agentId, agentInfo);
    
    console.log(`Agent ${agentId} registered with models:`, models);
    
    this.emit('agent:registered', agentInfo);

    // Confirm registration
    ws.send(JSON.stringify({
      type: 'registered',
      agentId,
      timestamp: Date.now(),
    }));
  }

  private updateHeartbeat(agentId: string) {
    const agent = this.agents.get(agentId);
    if (agent) {
      agent.lastHeartbeat = new Date();
    }
  }

  private handleToken(agentId: string, message: TokenChunk) {
    // Emit token to job handlers
    this.emit('token', {
      jobId: message.jobId,
      token: message.token,
      done: message.done,
      agentId,
    });
  }

  private handleJobComplete(agentId: string, jobId: string) {
    const agent = this.agents.get(agentId);
    if (agent) {
      agent.activeJobs.delete(jobId);
      if (agent.activeJobs.size === 0) {
        agent.status = 'idle';
      }
    }

    this.emit('job:complete', { jobId, agentId });
  }

  private handleJobError(agentId: string, message: any) {
    const agent = this.agents.get(agentId);
    if (agent) {
      agent.activeJobs.delete(message.jobId);
      if (agent.activeJobs.size === 0) {
        agent.status = 'idle';
      }
    }

    this.emit('job:error', {
      jobId: message.jobId,
      error: message.error,
      agentId,
    });
  }

  private updateAgentStatus(agentId: string, status: 'idle' | 'busy') {
    const agent = this.agents.get(agentId);
    if (agent) {
      agent.status = status;
    }
  }

  private checkHeartbeats() {
    const now = Date.now();
    const timeout = 30000; // 30 seconds

    for (const [agentId, agent] of this.agents.entries()) {
      const timeSinceHeartbeat = now - agent.lastHeartbeat.getTime();
      
      if (timeSinceHeartbeat > timeout) {
        console.log(`Agent ${agentId} timed out (no heartbeat for ${timeSinceHeartbeat}ms)`);
        agent.ws.close();
        this.agents.delete(agentId);
        this.emit('agent:timeout', agentId);
      }
    }
  }

  // Public API for job submission
  public submitJob(job: JobRequest): boolean {
    // Find an idle agent that supports this model
    const availableAgent = Array.from(this.agents.values()).find(
      agent => 
        agent.status === 'idle' && 
        (agent.models.length === 0 || agent.models.includes(job.model))
    );

    if (!availableAgent) {
      return false; // No available agent
    }

    // Send job to agent via WebSocket (INSTANT - NO POLLING)
    availableAgent.ws.send(JSON.stringify({
      type: 'job',
      jobId: job.jobId,
      model: job.model,
      prompt: job.prompt,
      parameters: job.parameters,
    }));

    availableAgent.activeJobs.add(job.jobId);
    availableAgent.status = 'busy';

    console.log(`Job ${job.jobId} assigned to agent ${availableAgent.id}`);
    
    return true;
  }

  public getAgentCount(): number {
    return this.agents.size;
  }

  public getIdleAgentCount(): number {
    return Array.from(this.agents.values()).filter(a => a.status === 'idle').length;
  }

  public close() {
    clearInterval(this.heartbeatInterval);
    this.wss.close();
  }
}
```

### Part 2: Updated Chat Route with SSE

```typescript
// server/routes/chat.ts

import express from 'express';
import crypto from 'crypto';
import { agentManager } from '../services/AgentConnectionManager';

const router = express.Router();

// Store active jobs
const activeJobs = new Map<string, {
  res: express.Response;
  buffer: string;
  startTime: number;
}>();

// Listen for tokens from agents
agentManager.on('token', (data) => {
  const job = activeJobs.get(data.jobId);
  if (!job) return;

  // Stream token immediately to client via SSE
  job.res.write(`data: ${JSON.stringify({
    token: data.token,
    done: data.done,
  })}\n\n`);

  if (data.done) {
    job.res.write('data: [DONE]\n\n');
    job.res.end();
    activeJobs.delete(data.jobId);
    
    const duration = Date.now() - job.startTime;
    console.log(`Job ${data.jobId} completed in ${duration}ms`);
  }
});

agentManager.on('job:error', (data) => {
  const job = activeJobs.get(data.jobId);
  if (!job) return;

  job.res.write(`data: ${JSON.stringify({
    error: data.error,
  })}\n\n`);
  job.res.end();
  activeJobs.delete(data.jobId);
});

// SSE streaming endpoint
router.post('/api/chat/completions', (req, res) => {
  const { messages, model } = req.body;
  const jobId = crypto.randomBytes(16).toString('hex');
  const lastMessage = messages[messages.length - 1].content;

  // Set SSE headers
  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache');
  res.setHeader('Connection', 'keep-alive');
  res.setHeader('X-Accel-Buffering', 'no'); // Disable nginx buffering

  // Register job
  activeJobs.set(jobId, {
    res,
    buffer: '',
    startTime: Date.now(),
  });

  // Submit to agent via WebSocket (INSTANT)
  const submitted = agentManager.submitJob({
    jobId,
    model,
    prompt: lastMessage,
    parameters: req.body.parameters,
  });

  if (!submitted) {
    res.write(`data: ${JSON.stringify({
      error: 'No available agents',
    })}\n\n`);
    res.end();
    activeJobs.delete(jobId);
    return;
  }

  // Handle client disconnect
  req.on('close', () => {
    activeJobs.delete(jobId);
    console.log(`Client disconnected for job ${jobId}`);
  });
});

// Health endpoint
router.get('/api/agents/status', (req, res) => {
  res.json({
    total: agentManager.getAgentCount(),
    idle: agentManager.getIdleAgentCount(),
    busy: agentManager.getAgentCount() - agentManager.getIdleAgentCount(),
  });
});

export default router;
```

### Part 3: Python Agent with WebSocket

```python
# agent.py (COMPLETE REWRITE)

import asyncio
import json
import websockets
import aiohttp
import os
from typing import Optional

class OllamaAgent:
    def __init__(
        self,
        server_url: str = "ws://localhost:8081",
        ollama_url: str = "http://localhost:11434",
        models: list = None
    ):
        self.server_url = server_url
        self.ollama_url = ollama_url
        self.models = models or []
        self.agent_id: Optional[str] = None
        self.ws: Optional[websockets.WebSocketClientProtocol] = None
        self.running = False
        
    async def connect(self):
        """Connect to server via WebSocket"""
        print(f"Connecting to {self.server_url}...")
        
        try:
            self.ws = await websockets.connect(self.server_url)
            self.running = True
            print("Connected to server!")
            
            # Wait for welcome message
            welcome = await self.ws.recv()
            welcome_data = json.loads(welcome)
            
            if welcome_data.get('type') == 'welcome':
                self.agent_id = welcome_data['agentId']
                print(f"Assigned agent ID: {self.agent_id}")
            
            # Register with server
            await self.register()
            
            # Start heartbeat task
            asyncio.create_task(self.heartbeat_loop())
            
            # Start message handling
            await self.message_loop()
            
        except Exception as e:
            print(f"Connection error: {e}")
            self.running = False
            
    async def register(self):
        """Register agent with available models"""
        await self.ws.send(json.dumps({
            'type': 'register',
            'models': self.models,
        }))
        print(f"Registered with models: {self.models}")
        
    async def heartbeat_loop(self):
        """Send heartbeat every 5 seconds"""
        while self.running:
            try:
                await asyncio.sleep(5)
                if self.ws and not self.ws.closed:
                    await self.ws.send(json.dumps({
                        'type': 'heartbeat',
                        'timestamp': asyncio.get_event_loop().time(),
                    }))
            except Exception as e:
                print(f"Heartbeat error: {e}")
                break
                
    async def message_loop(self):
        """Handle incoming messages from server"""
        try:
            async for message in self.ws:
                data = json.loads(message)
                message_type = data.get('type')
                
                if message_type == 'job':
                    # New job received - process immediately
                    asyncio.create_task(self.process_job(data))
                elif message_type == 'registered':
                    print("Registration confirmed")
                else:
                    print(f"Unknown message type: {message_type}")
                    
        except websockets.exceptions.ConnectionClosed:
            print("Connection closed by server")
            self.running = False
        except Exception as e:
            print(f"Message loop error: {e}")
            self.running = False
            
    async def process_job(self, job_data: dict):
        """Process a job by calling Ollama and streaming tokens back"""
        job_id = job_data['jobId']
        model = job_data['model']
        prompt = job_data['prompt']
        parameters = job_data.get('parameters', {})
        
        print(f"Processing job {job_id} with model {model}")
        
        # Update status to busy
        await self.ws.send(json.dumps({
            'type': 'status',
            'status': 'busy',
        }))
        
        try:
            # Call Ollama with streaming
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        'model': model,
                        'prompt': prompt,
                        'stream': True,
                        **parameters,
                    }
                ) as response:
                    # Stream tokens back via WebSocket
                    async for line in response.content:
                        if not line:
                            continue
                            
                        try:
                            chunk = json.loads(line)
                            token = chunk.get('response', '')
                            done = chunk.get('done', False)
                            
                            if token:
                                # Send token immediately via WebSocket
                                # NO HTTP POST OVERHEAD!
                                await self.ws.send(json.dumps({
                                    'type': 'token',
                                    'jobId': job_id,
                                    'token': token,
                                    'done': done,
                                }))
                            
                            if done:
                                break
                                
                        except json.JSONDecodeError:
                            continue
            
            # Job complete
            await self.ws.send(json.dumps({
                'type': 'job_complete',
                'jobId': job_id,
            }))
            
            print(f"Job {job_id} completed")
            
        except Exception as e:
            print(f"Job {job_id} error: {e}")
            
            # Send error
            await self.ws.send(json.dumps({
                'type': 'job_error',
                'jobId': job_id,
                'error': str(e),
            }))
        
        finally:
            # Update status to idle
            await self.ws.send(json.dumps({
                'type': 'status',
                'status': 'idle',
            }))

async def main():
    # Configuration
    server_url = os.getenv('SERVER_WS_URL', 'ws://localhost:8081')
    ollama_url = os.getenv('OLLAMA_URL', 'http://localhost:11434')
    models = os.getenv('MODELS', 'llama2,mistral').split(',')
    
    agent = OllamaAgent(
        server_url=server_url,
        ollama_url=ollama_url,
        models=models
    )
    
    # Keep trying to connect
    while True:
        try:
            await agent.connect()
        except Exception as e:
            print(f"Error: {e}")
            print("Reconnecting in 5 seconds...")
            await asyncio.sleep(5)

if __name__ == "__main__":
    asyncio.run(main())
```

### Part 4: Server Initialization

```typescript
// server/index.ts

import express from 'express';
import cors from 'cors';
import { AgentConnectionManager } from './services/AgentConnectionManager';
import chatRouter from './routes/chat';

const app = express();

app.use(cors());
app.use(express.json());

// Initialize Agent Connection Manager
export const agentManager = new AgentConnectionManager(8081);

// Mount routes
app.use(chatRouter);

const PORT = process.env.PORT || 3000;

app.listen(PORT, () => {
  console.log(`Server listening on port ${PORT}`);
  console.log(`Agent WebSocket server on port 8081`);
});

// Graceful shutdown
process.on('SIGTERM', () => {
  agentManager.close();
  process.exit(0);
});
```

---

## Performance Comparison

### Before (Current Architecture)
```
First Token Timeline:
0ms: User clicks send
0-500ms: Agent polling wait (RANDOM)
500-2000ms: Ollama generation
2080ms: First token (80ms HTTP overhead)
2160ms: Second token (80ms HTTP overhead)
2240ms: Third token (80ms HTTP overhead)
...every subsequent token adds 80-120ms
```

**Total to first token:** 2000-2500ms

### After (WebSocket Streaming)
```
First Token Timeline:
0ms: User clicks send
0ms: Job pushed to agent (INSTANT)
0-1500ms: Ollama generation
1500ms: First token (instant via WebSocket)
1550ms: Second token (instant)
1600ms: Third token (instant)
...tokens flow at Ollama's natural speed
```

**Total to first token:** 1000-1500ms (only Ollama)

**Performance Improvement:**
- **First token:** 40-50% faster
- **Token streaming:** 80-120ms → 0ms (100% improvement)
- **User experience:** Smooth, ChatGPT-like streaming

---

## Implementation Steps

### Week 1: Setup WebSocket Infrastructure

1. **Install dependencies:**
```bash
npm install ws @types/ws
pip install websockets aiohttp
```

2. **Add AgentConnectionManager**
- Copy `AgentConnectionManager.ts` to `server/services/`
- Initialize in `server/index.ts`

3. **Test WebSocket connection**
```bash
# Terminal 1: Start server
npm run dev

# Terminal 2: Start agent
python agent.py

# Should see: "Agent registered with models: [...]"
```

### Week 2: Update Chat Route

1. **Replace current chat route** with new SSE version
2. **Remove old HTTP POST endpoints** for token submission
3. **Test streaming** with curl:
```bash
curl -N http://localhost:3000/api/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Hello"}],
    "model": "llama2"
  }'
```

### Week 3: Frontend Integration

1. **No changes needed!** Frontend still uses SSE
2. **Optional:** Add agent status display
3. **Test with multiple concurrent users**

---

## Monitoring & Debugging

### Agent Status Endpoint

```bash
curl http://localhost:3000/api/agents/status

# Response:
{
  "total": 3,
  "idle": 2,
  "busy": 1
}
```

### WebSocket Message Flow

```
Server → Agent:
{
  "type": "job",
  "jobId": "abc123",
  "model": "llama2",
  "prompt": "Hello world"
}

Agent → Server (streaming):
{
  "type": "token",
  "jobId": "abc123",
  "token": "Hello",
  "done": false
}
{
  "type": "token",
  "jobId": "abc123",
  "token": " there",
  "done": false
}
{
  "type": "token",
  "jobId": "abc123",
  "token": "!",
  "done": true
}
{
  "type": "job_complete",
  "jobId": "abc123"
}
```

---

## Expected Results

**Bottlenecks Eliminated:**
✅ Agent polling delay: 500ms → 0ms
✅ HTTP POST overhead: 80-120ms → 0ms
✅ Total improvement: ~40-50% faster first token, smooth streaming

**User Experience:**
✅ Tokens appear immediately as generated
✅ Smooth character-by-character rendering
✅ Matches Gradient/ChatGPT quality
✅ No more stuttering or delays

**System Benefits:**
✅ Simpler architecture (no HTTP polling)
✅ Better scalability (persistent connections)
✅ Real-time agent monitoring
✅ Automatic reconnection handling

Give this complete solution to your engineering team - it eliminates both critical bottlenecks!