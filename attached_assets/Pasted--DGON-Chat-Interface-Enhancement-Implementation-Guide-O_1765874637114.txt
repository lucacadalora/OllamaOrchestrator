# DGON Chat Interface Enhancement - Implementation Guide

## Overview
This guide will help your engineers transform OllamaOrchestrator to match the chat.gradient.network interface quality.

## Current Stack Analysis
Based on your repository:
- Frontend: TypeScript/React (client/)
- Backend: Node.js/Express (server/)
- Database: Drizzle ORM
- Agent: Python (agent.py)

---

## Phase 1: UI Component Library Setup

### 1.1 Install Required Dependencies

```bash
# Navigate to your project
cd OllamaOrchestrator

# Install shadcn/ui components (if not already installed)
npx shadcn-ui@latest init

# Install additional dependencies
npm install lucide-react class-variance-authority clsx tailwind-merge
npm install @radix-ui/react-dialog @radix-ui/react-dropdown-menu
npm install date-fns
```

### 1.2 Configure Tailwind CSS

Update `tailwind.config.ts`:

```typescript
import type { Config } from "tailwindcss"

const config = {
  darkMode: ["class"],
  content: [
    './pages/**/*.{ts,tsx}',
    './components/**/*.{ts,tsx}',
    './app/**/*.{ts,tsx}',
    './src/**/*.{ts,tsx}',
    './client/**/*.{ts,tsx}',
  ],
  theme: {
    extend: {
      colors: {
        border: "hsl(var(--border))",
        input: "hsl(var(--input))",
        ring: "hsl(var(--ring))",
        background: "hsl(var(--background))",
        foreground: "hsl(var(--foreground))",
        primary: {
          DEFAULT: "hsl(var(--primary))",
          foreground: "hsl(var(--primary-foreground))",
        },
        secondary: {
          DEFAULT: "hsl(var(--secondary))",
          foreground: "hsl(var(--secondary-foreground))",
        },
        muted: {
          DEFAULT: "hsl(var(--muted))",
          foreground: "hsl(var(--muted-foreground))",
        },
      },
      borderRadius: {
        lg: "var(--radius)",
        md: "calc(var(--radius) - 2px)",
        sm: "calc(var(--radius) - 4px)",
      },
      keyframes: {
        "bounce": {
          "0%, 100%": { transform: "translateY(0)" },
          "50%": { transform: "translateY(-5px)" },
        },
      },
      animation: {
        "bounce": "bounce 0.6s ease-in-out infinite",
      },
    },
  },
  plugins: [require("tailwindcss-animate")],
} satisfies Config

export default config
```

---

## Phase 2: Backend API Modifications

### 2.1 Update Chat API Endpoint

File: `server/routes/chat.ts` (or create if doesn't exist)

```typescript
import express from 'express';
import { StreamingTextResponse } from 'ai';

const router = express.Router();

// Chat completion endpoint with streaming
router.post('/api/chat', async (req, res) => {
  const { messages, model, stream = true } = req.body;

  try {
    // Get the last user message
    const lastMessage = messages[messages.length - 1];
    
    if (stream) {
      // Set headers for SSE (Server-Sent Events)
      res.setHeader('Content-Type', 'text/event-stream');
      res.setHeader('Cache-Control', 'no-cache');
      res.setHeader('Connection', 'keep-alive');

      // Call your Ollama orchestrator
      const response = await fetch('http://your-ollama-orchestrator/api/generate', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: model,
          prompt: lastMessage.content,
          stream: true,
        }),
      });

      // Stream the response
      const reader = response.body?.getReader();
      const decoder = new TextDecoder();

      while (true) {
        const { done, value } = await reader!.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n').filter(line => line.trim());

        for (const line of lines) {
          try {
            const json = JSON.parse(line);
            if (json.response) {
              res.write(`data: ${JSON.stringify({ token: json.response })}\n\n`);
            }
          } catch (e) {
            // Skip invalid JSON
          }
        }
      }

      res.write('data: [DONE]\n\n');
      res.end();
    } else {
      // Non-streaming response
      const response = await fetch('http://your-ollama-orchestrator/api/generate', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: model,
          prompt: lastMessage.content,
          stream: false,
        }),
      });

      const data = await response.json();
      res.json({ response: data.response });
    }
  } catch (error) {
    console.error('Chat error:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
});

export default router;
```

### 2.2 Add Node Status API

File: `server/routes/nodes.ts`

```typescript
import express from 'express';

const router = express.Router();

interface NodeStatus {
  id: string;
  gpu: string;
  location: string;
  status: 'online' | 'offline' | 'busy';
  load: number;
  model: string;
  layerStart: number;
  layerEnd: number;
  totalLayers: number;
  blocksServed: number;
}

// Get all nodes status
router.get('/api/nodes', async (req, res) => {
  try {
    // Fetch from your Ollama orchestrator
    const response = await fetch('http://your-ollama-orchestrator/api/nodes');
    const nodes = await response.json();

    // Transform to match frontend interface
    const transformedNodes: NodeStatus[] = nodes.map((node: any) => ({
      id: node.id,
      gpu: node.gpu_name || 'Unknown GPU',
      location: node.location || 'Unknown',
      status: node.status,
      load: node.load || 0,
      model: node.model,
      layerStart: node.layer_start || 0,
      layerEnd: node.layer_end || 0,
      totalLayers: node.total_layers || 0,
      blocksServed: node.blocks_served || 0,
    }));

    res.json(transformedNodes);
  } catch (error) {
    console.error('Nodes fetch error:', error);
    res.status(500).json({ error: 'Failed to fetch nodes' });
  }
});

// Get specific node status
router.get('/api/nodes/:id', async (req, res) => {
  try {
    const { id } = req.params;
    const response = await fetch(`http://your-ollama-orchestrator/api/nodes/${id}`);
    const node = await response.json();
    res.json(node);
  } catch (error) {
    res.status(500).json({ error: 'Failed to fetch node' });
  }
});

export default router;
```

### 2.3 Add Models API

File: `server/routes/models.ts`

```typescript
import express from 'express';

const router = express.Router();

interface ModelInfo {
  id: string;
  name: string;
  size: string;
  nodes: number;
  activeNodes: number;
  avgLoad: number;
  status: 'available' | 'unavailable';
}

// Get available models
router.get('/api/models', async (req, res) => {
  try {
    // Fetch from Ollama orchestrator
    const response = await fetch('http://your-ollama-orchestrator/api/models');
    const models = await response.json();

    // Transform to match frontend interface
    const transformedModels: ModelInfo[] = models.map((model: any) => ({
      id: model.id,
      name: model.name,
      size: model.size,
      nodes: model.total_nodes || 0,
      activeNodes: model.active_nodes || 0,
      avgLoad: model.avg_load || 0,
      status: model.active_nodes > 0 ? 'available' : 'unavailable',
    }));

    res.json(transformedModels);
  } catch (error) {
    console.error('Models fetch error:', error);
    res.status(500).json({ error: 'Failed to fetch models' });
  }
});

export default router;
```

---

## Phase 3: Frontend Integration

### 3.1 Create Main Chat Page

File: `client/src/pages/ChatPage.tsx`

```typescript
import React, { useState, useEffect } from 'react';
import { EnhancedChatInterface } from '../components/EnhancedChatInterface';
import { InferenceBackendPanel } from '../components/InferenceBackendPanel';
import { useNodes } from '../hooks/useNodes';
import { useModels } from '../hooks/useModels';

export function ChatPage() {
  const [selectedModel, setSelectedModel] = useState<string>('');
  const [showBackendPanel, setShowBackendPanel] = useState(false);
  
  const { nodes, loading: nodesLoading } = useNodes();
  const { models, loading: modelsLoading } = useModels();

  useEffect(() => {
    // Auto-select first available model
    if (models.length > 0 && !selectedModel) {
      setSelectedModel(models[0].id);
    }
  }, [models]);

  const handleSendMessage = async (message: string): Promise<string> => {
    const response = await fetch('/api/chat', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        messages: [{ role: 'user', content: message }],
        model: selectedModel,
        stream: false,
      }),
    });

    const data = await response.json();
    return data.response;
  };

  // Filter nodes for selected model
  const modelNodes = nodes.filter(node => node.model === selectedModel);

  return (
    <div className="relative h-screen">
      <EnhancedChatInterface
        modelName={models.find(m => m.id === selectedModel)?.name || 'Select a model'}
        onSendMessage={handleSendMessage}
      />

      <InferenceBackendPanel
        modelName={models.find(m => m.id === selectedModel)?.name || ''}
        totalWorkers={modelNodes.length}
        workers={modelNodes}
        isOpen={showBackendPanel}
        onClose={() => setShowBackendPanel(false)}
      />

      {/* Floating button to show backend panel */}
      <button
        onClick={() => setShowBackendPanel(true)}
        className="fixed bottom-6 right-6 px-4 py-2 bg-white border rounded-lg shadow-lg hover:shadow-xl transition-all flex items-center gap-2"
      >
        <svg className="w-4 h-4" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2">
          <path d="M12 2v20M17 5H9.5a3.5 3.5 0 0 0 0 7h5a3.5 3.5 0 0 1 0 7H6"/>
        </svg>
        View Network
      </button>
    </div>
  );
}
```

### 3.2 Create Custom Hooks

File: `client/src/hooks/useNodes.ts`

```typescript
import { useState, useEffect } from 'react';

interface NodeStatus {
  id: string;
  gpu: string;
  location: string;
  status: 'online' | 'offline' | 'busy' | 'completed' | 'processing' | 'idle';
  load: number;
  model: string;
  layerStart: number;
  layerEnd: number;
  totalLayers: number;
  blocksServed: number;
}

export function useNodes() {
  const [nodes, setNodes] = useState<NodeStatus[]>([]);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState<Error | null>(null);

  useEffect(() => {
    const fetchNodes = async () => {
      try {
        const response = await fetch('/api/nodes');
        if (!response.ok) throw new Error('Failed to fetch nodes');
        const data = await response.json();
        setNodes(data);
      } catch (err) {
        setError(err as Error);
      } finally {
        setLoading(false);
      }
    };

    fetchNodes();
    
    // Poll every 5 seconds
    const interval = setInterval(fetchNodes, 5000);
    
    return () => clearInterval(interval);
  }, []);

  return { nodes, loading, error };
}
```

File: `client/src/hooks/useModels.ts`

```typescript
import { useState, useEffect } from 'react';

interface ModelInfo {
  id: string;
  name: string;
  size: string;
  nodes: number;
  activeNodes: number;
  avgLoad: number;
  status: 'available' | 'unavailable';
}

export function useModels() {
  const [models, setModels] = useState<ModelInfo[]>([]);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState<Error | null>(null);

  useEffect(() => {
    const fetchModels = async () => {
      try {
        const response = await fetch('/api/models');
        if (!response.ok) throw new Error('Failed to fetch models');
        const data = await response.json();
        setModels(data);
      } catch (err) {
        setError(err as Error);
      } finally {
        setLoading(false);
      }
    };

    fetchModels();
    
    // Poll every 10 seconds
    const interval = setInterval(fetchModels, 10000);
    
    return () => clearInterval(interval);
  }, []);

  return { models, loading, error };
}
```

---

## Phase 4: Python Agent Integration

### 4.1 Update agent.py

Your `agent.py` needs to expose node status information:

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import subprocess
import json

app = FastAPI()

class NodeStatus(BaseModel):
    id: str
    gpu_name: str
    location: str
    status: str
    load: float
    model: str
    layer_start: int
    layer_end: int
    total_layers: int
    blocks_served: int

@app.get("/api/nodes")
async def get_nodes():
    """Get status of all connected Ollama nodes"""
    try:
        # Example: Query your Ollama orchestrator
        # This is pseudo-code - adjust to your actual implementation
        result = subprocess.run(
            ['ollama', 'list'], 
            capture_output=True, 
            text=True
        )
        
        # Parse and return node information
        nodes = []
        # Parse result.stdout and create NodeStatus objects
        
        return nodes
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/models")
async def get_models():
    """Get available models across all nodes"""
    try:
        result = subprocess.run(
            ['ollama', 'list'], 
            capture_output=True, 
            text=True
        )
        
        # Parse and return model information
        models = []
        
        return models
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

## Phase 5: File Structure

```
OllamaOrchestrator/
├── client/
│   ├── src/
│   │   ├── components/
│   │   │   ├── EnhancedChatInterface.tsx  ← New
│   │   │   ├── InferenceBackendPanel.tsx  ← New
│   │   │   ├── ModelSelectionCard.tsx     ← New
│   │   │   └── ui/                        ← shadcn components
│   │   ├── hooks/
│   │   │   ├── useNodes.ts                ← New
│   │   │   └── useModels.ts               ← New
│   │   ├── pages/
│   │   │   └── ChatPage.tsx               ← New
│   │   └── lib/
│   │       └── utils.ts
│   └── package.json
├── server/
│   ├── routes/
│   │   ├── chat.ts                        ← New
│   │   ├── nodes.ts                       ← New
│   │   └── models.ts                      ← New
│   └── index.ts
├── agent.py                               ← Modified
└── package.json
```

---

## Phase 6: Testing Checklist

### Frontend Testing
- [ ] Chat interface loads correctly
- [ ] Messages send and receive properly
- [ ] Streaming responses work (if implemented)
- [ ] Copy button works on messages
- [ ] Inference backend panel opens/closes
- [ ] Node visualization displays correctly
- [ ] Model selection works
- [ ] Responsive design works on mobile

### Backend Testing
- [ ] `/api/chat` endpoint returns responses
- [ ] `/api/nodes` endpoint returns node status
- [ ] `/api/models` endpoint returns models
- [ ] Streaming works (if implemented)
- [ ] Error handling works properly

### Integration Testing
- [ ] Node status updates in real-time
- [ ] Model switching works
- [ ] Multi-node visualization is accurate
- [ ] Performance is acceptable

---

## Phase 7: Deployment

### Environment Variables

Create `.env` file:

```bash
# API Configuration
OLLAMA_ORCHESTRATOR_URL=http://your-orchestrator-url
API_PORT=3000

# Frontend
VITE_API_URL=http://localhost:3000

# Database (if needed)
DATABASE_URL=postgresql://user:password@localhost:5432/dgon
```

### Build Commands

```bash
# Install dependencies
npm install

# Build frontend
cd client && npm run build

# Start production server
npm run start
```

---

## Phase 8: Additional Enhancements (Optional)

### 8.1 Add Markdown Support

```bash
npm install react-markdown remark-gfm rehype-highlight
```

### 8.2 Add Code Syntax Highlighting

```bash
npm install highlight.js
```

### 8.3 Add Streaming Support

Use Server-Sent Events (SSE) or WebSockets for real-time streaming.

---

## Troubleshooting

### Common Issues

1. **Components not found**: Make sure shadcn/ui is properly installed
2. **Styling not applying**: Check Tailwind CSS configuration
3. **API not responding**: Verify backend URLs in environment variables
4. **Node status not updating**: Check polling intervals in hooks

---

## Next Steps After Implementation

1. Test thoroughly with real Ollama nodes
2. Add user authentication
3. Implement chat history persistence
4. Add rate limiting
5. Set up monitoring and analytics
6. Deploy to production

---

## Support

If your engineers have questions during implementation:
1. Check the component files provided
2. Refer to shadcn/ui documentation: https://ui.shadcn.com
3. Review Tailwind CSS docs: https://tailwindcss.com
4. Test with example data first before connecting real backends